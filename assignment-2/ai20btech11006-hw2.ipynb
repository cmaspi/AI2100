{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Markdown cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import statements\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.029446844526784283\n",
      "1.0294468445267844\n"
     ]
    }
   ],
   "source": [
    "def entropy(pmf: np.array) -> float:\n",
    "    \"\"\"This function returns the entropy of the given pmf \n",
    "\n",
    "    The formula used is : -sum p(x) log p(x)\n",
    "\n",
    "    Example\n",
    "    --------\n",
    "    >>> p = np.array([0.5, 0.5])\n",
    "    >>> entropy(p)\n",
    "    1.0\n",
    "    \"\"\"\n",
    "    return -np.sum(pmf*np.log2(pmf))\n",
    "\n",
    "def KL_divergence(p: np.array, q: np.array) -> float:\n",
    "    \"\"\"This function returns the KL divergence between the two given pmfs\n",
    "    \n",
    "    The formula used is : sum p(x) log p(x)/q(x) \n",
    "    \n",
    "    Example\n",
    "    -------\n",
    "    >>> p = np.array([0.5, 0.5])\n",
    "    >>> q = np.array([0.6, 0.4])\n",
    "    >>> KL_divergence(p, q)\n",
    "    0.029446844526784283\n",
    "    \"\"\"\n",
    "    idx = np.argwhere( q == 0.0 )\n",
    "    q[idx]+=1e-5\n",
    "    idx = np.argwhere( p == 0.0 )\n",
    "    p[idx]+=1e-5\n",
    "    return np.sum(p * np.log2(p/q)) \n",
    "\n",
    "def cross_entropy(p: np.array, q:np.array) -> float:\n",
    "    \"\"\"This function returns the cross entropy betwen the two given pmfs\n",
    "    \n",
    "    The formula used is : H(p) + D(p||q)\n",
    "    \n",
    "    Example\n",
    "    -------\n",
    "    >>> p = np.array([0.5, 0.5])\n",
    "    >>> q = np.array([0.6, 0.4])\n",
    "    >>> cross_entropy(p, q)\n",
    "    1.0294468445267844\n",
    "    \"\"\"\n",
    "    return entropy(p)+KL_divergence(p, q)\n",
    "\n",
    "def JS_divergence(p: np.array, q:np.array) -> float:\n",
    "    \"\"\"This function return the Jenson Shannon divergence between\n",
    "    the given two pmfs\n",
    "    \n",
    "    The formula used is : D(p||m)+D(q||m), where\n",
    "    m = (p+q)/2\n",
    "    \n",
    "    Example\n",
    "    -------\n",
    "    >>> p = np.array([0.5, 0.5])\n",
    "    >>> q = np.array([0.6, 0.4])\n",
    "    >>> JS_divergence(p, q)\n",
    "    0.014598313520947925\n",
    "    \"\"\"\n",
    "    m = (p+q)/2\n",
    "    return KL_divergence(p, m) + KL_divergence(q, m)\n",
    "\n",
    "\n",
    "\n",
    "p = np.array([0.5, 0.5])\n",
    "q = np.array([0.6, 0.4])\n",
    "print(entropy(p))\n",
    "print(KL_divergence(p, q))\n",
    "print(cross_entropy(p, q))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.014598313520947925"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "JS_divergence(p,q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
