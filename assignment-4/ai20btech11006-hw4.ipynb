{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've taken help from <a href=\"https://towardsdatascience.com/building-neural-network-from-scratch-9c88535bf8e9\"> blog on MLP </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cmaspi/.local/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# importing libraries\n",
    "import numpy as np\n",
    "import typing as ty\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('dark_background')\n",
    "\n",
    "# setting seed\n",
    "np.random.seed(0)\n",
    "\n",
    "def dataloader():\n",
    "    train_ = torchvision.datasets.MNIST(\"files/\")\n",
    "\n",
    "    train_dict = { i:[] for i in range(10) }\n",
    "\n",
    "    for image, label in train_:\n",
    "        train_dict[label].append(np.array(image).reshape(28,28,1)/1.)\n",
    "\n",
    "    # 1K images, 100 for each label\n",
    "    train = []\n",
    "    label = []\n",
    "    for i in range(10):\n",
    "        label.extend([i]*100)\n",
    "        train.extend(train_dict[i][100:200])\n",
    "    y_train = np.array(label)\n",
    "    X_train = np.array(train)\n",
    "\n",
    "    # validation set\n",
    "    train = []\n",
    "    label = []\n",
    "    for i in range(10):\n",
    "        label.extend([i]*10)\n",
    "        train.extend(train_dict[i][400:410])\n",
    "    y_val = np.array(label)\n",
    "    X_val = np.array(train)\n",
    "\n",
    "    return X_train, y_train, X_val, y_val\n",
    "    \n",
    "const = 0.02\n",
    "X_train, y_train, X_val, y_val = dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train[0], cmap = 'gray')\n",
    "plt.title(f'label : {y_train[0]}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross entropy loss\n",
    "\n",
    "def cross_entropy(p: np.ndarray, q:np.ndarray) -> float:\n",
    "    \"\"\"This function returns the cross entropy betwen the two given pmfs\n",
    "    \n",
    "    The formula used is : - sum p log q\n",
    "    \"\"\"\n",
    "    return -np.mean(np.log(q[np.arange(p.shape[0]),p]))\n",
    "\n",
    "# cross_entropy_util = lambda \n",
    "\n",
    "\n",
    "# activation functions\n",
    "def relu(x : ty.Union[float, np.ndarray]) -> ty.Union[float, np.ndarray]:\n",
    "    \"\"\"ReLU function\n",
    "    \n",
    "    returns max(0,x)\"\"\"\n",
    "    return np.maximum(0,x)\n",
    "\n",
    "def softmax(x : np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Softmax function\n",
    "    \n",
    "    return [e^x_i/sum(e^x_i) for all i]\"\"\"\n",
    "    denom = np.sum(np.exp(x), axis=-1)\n",
    "    return np.exp(x)/denom[:,None]\n",
    "\n",
    "# max pooling\n",
    "max_pool = lambda x: np.max(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def forward(self, input : np.ndarray) -> np.ndarray:\n",
    "        return input\n",
    "    \n",
    "    def backprop(self, input : np.ndarray, grad_in):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv2D(Layer):\n",
    "    def __init__(self, num_kernels, kernel_shape, learning_rate : int = 0.001, momentum = False, eta = 0.9, rmsprop = False, rho = 0.9) -> None:\n",
    "        self.num_kernels = num_kernels\n",
    "        self.kernel_shape = kernel_shape\n",
    "        self.learning_rate = learning_rate\n",
    "        self.prev_change = [np.zeros(kernel_shape) for _ in range(self.num_kernels)]\n",
    "        self.kernels = []\n",
    "        self.padding = 2\n",
    "        self.rmsprop = rmsprop\n",
    "        self.rho = rho\n",
    "        \n",
    "        for _ in range(self.num_kernels):\n",
    "            k = np.random.normal(0, np.sqrt(2/np.product(kernel_shape)), kernel_shape)\n",
    "            self.kernels.append(k)\n",
    "        self.momentum = momentum\n",
    "        self.eta = eta\n",
    "        self.epsilon = 1e-9\n",
    "\n",
    "    def forward(self, input : np.ndarray):\n",
    "        padding = self.padding\n",
    "        output = []\n",
    "        for img in input:\n",
    "            k = self.kernel_shape[0]\n",
    "\n",
    "            horizontal_flipped = img[::-1]\n",
    "            vertical_flipped = img[:,::-1]\n",
    "            fully_flipped = img[::-1,::-1]\n",
    "            W,H,c = img.shape\n",
    "            padded_array = np.zeros((W+2*padding, H+2*padding,c))\n",
    "            padded_array[padding:-padding,padding:-padding] = img.copy()\n",
    "            padded_array[:padding,:padding] = fully_flipped[-padding:,-padding:]\n",
    "            padded_array[-padding:,-padding:] = fully_flipped[:padding,:padding]\n",
    "            padded_array[:padding,-padding:] = fully_flipped[-padding:,:padding]\n",
    "            padded_array[-padding:,:padding] = fully_flipped[:padding,-padding:]\n",
    "            padded_array[:padding,padding:-padding] = horizontal_flipped[-padding:,:]\n",
    "            padded_array[-padding:,padding:-padding] = horizontal_flipped[:padding,:]\n",
    "            padded_array[padding:-padding,:padding] = vertical_flipped[:,-padding:]\n",
    "            padded_array[padding:-padding,-padding:] = vertical_flipped[:,:padding]\n",
    "\n",
    "            out = []\n",
    "            for kernel in self.kernels:\n",
    "                x,y = 0, 0\n",
    "                temp = np.zeros(img.shape[:2])\n",
    "                # print(temp.shape)\n",
    "                for i in range(W+1+2*padding-k,):\n",
    "                    for j in range(H+1+2*padding-k):\n",
    "                        temp[x, y] = np.sum(padded_array[i:i+k,j:j+k,:]*kernel)\n",
    "                        y+=1\n",
    "                    x+=1\n",
    "                    y=0\n",
    "                out.append(temp)                \n",
    "            out = np.dstack(out)        \n",
    "            output.append(out)\n",
    "        output = np.array(output)\n",
    "        return output\n",
    "\n",
    "    def backprop(self, input : np.ndarray, grad_out):\n",
    "        \"\"\"\n",
    "        What is being done\n",
    "        ------------------\n",
    "        Say we have 14 x 14 x 4 image and 4 (5 x 5 x 4) kernels\n",
    "        We take the grad_out which is 14 x 14 x 4 array and multiply it with\n",
    "        each kernel to make 18 x 18 x 4 array for each kernel,\n",
    "        We will further take only the central portion\n",
    "        or just fold the array on edges\n",
    "\n",
    "        to update the kernels, we will use the fact that\n",
    "        dL/dK = dL/dO * I \n",
    "\n",
    "        so basically, we have dL/dO as grad_out\n",
    "        the dimensions are N x 14 x 14 x 4\n",
    "        while for input, we have dimensions N x 14 x 14 x 4\n",
    "        so for kernel 1\n",
    "        dL/dk1 =    I      {conv}      dL/dO\n",
    "                N x 18 x 18 x 4 ||| N x 14 x 14 x 4\n",
    "                              ^                   ^\n",
    "                       number of channels | number of kernels\n",
    "\n",
    "        Notice that I is the padded image,\n",
    "        we will take convolution of each channel in input with each output corresponding to the kernel.\n",
    "        so we will convolute\n",
    "        18 x 18 with 14 x 14, then for each channel when we do this and stack them\n",
    "        we will get 5 x 5 x 4, this is for one kernel, we will do this for all 4 kernels\n",
    "\n",
    "        ----------------\n",
    "        For the cases of 28 x 28 x 1, and 4 kernels 5 x 5 x 1\n",
    "        we take the grad_out which is 28 x 28 x 4, and multiply it with each kernel\n",
    "        \"\"\"\n",
    "        padding = self.padding\n",
    "        \n",
    "        N, W, H, _ = input.shape\n",
    "        C = self.kernels.__len__()\n",
    "        grad_in_overall = []\n",
    "        for i in range(N):\n",
    "            grad_in = np.zeros_like(input[i])\n",
    "            for c in range(C):\n",
    "                dxk = np.zeros((W+2*padding, H+2*padding, input[0].shape[-1]))\n",
    "                for xx in range(W):\n",
    "                    for yy in range(H):\n",
    "                        dxk[xx:xx+2*padding+1,yy:yy+2*padding+1,:] += grad_out[i, xx,yy, c] * self.kernels[c]\n",
    "                grad_in += dxk[padding:-padding,padding:-padding,:]\n",
    "            grad_in_overall.append(grad_in)\n",
    "        N, W, H, C = input.shape\n",
    "        num = grad_out.shape[-1] # number of kernels\n",
    "\n",
    "        dk = [np.zeros(self.kernel_shape) for i in range(num)]\n",
    "\n",
    "        for i in range(N):\n",
    "            img = input[i]\n",
    "            K = self.kernel_shape[0]\n",
    "            horizontal_flipped = img[::-1]\n",
    "            vertical_flipped = img[:,::-1]\n",
    "            fully_flipped = img[::-1,::-1]\n",
    "            W,H,c = img.shape\n",
    "            padded_array = np.zeros((W+2*padding, H+2*padding,c))\n",
    "            padded_array[padding:-padding,padding:-padding] = img.copy()\n",
    "            padded_array[:padding,:padding] = fully_flipped[-padding:,-padding:]\n",
    "            padded_array[-padding:,-padding:] = fully_flipped[:padding,:padding]\n",
    "            padded_array[:padding,-padding:] = fully_flipped[-padding:,:padding]\n",
    "            padded_array[-padding:,:padding] = fully_flipped[:padding,-padding:]\n",
    "            padded_array[:padding,padding:-padding] = horizontal_flipped[-padding:,:]\n",
    "            padded_array[-padding:,padding:-padding] = horizontal_flipped[:padding,:]\n",
    "            padded_array[padding:-padding,:padding] = vertical_flipped[:,-padding:]\n",
    "            padded_array[padding:-padding,-padding:] = vertical_flipped[:,:padding]\n",
    "            # padded_array is 18 x 18 x 4\n",
    "            for k in range(num): # this is the index of kernel\n",
    "                for c in range(C): # this is index of channels\n",
    "                    # remember that we will take convolution of this single channel matrix\n",
    "                    # with our grad_output for different kernels\n",
    "                    for xx in range(2*padding+1):\n",
    "                        for yy in range(2*padding+1):\n",
    "                            dk[k][xx,yy,c] += np.sum(padded_array[xx:xx+W, yy:yy+H,c]*grad_out[i,:,:,k])*5\n",
    "        for k in range(num):\n",
    "            if not self.rmsprop:\n",
    "                self.kernels[k] -= dk[k]*self.learning_rate\n",
    "            else:\n",
    "                self.prev_change[k] = self.rho*self.prev_change[k] + (1-self.rho) * dk[k]**2\n",
    "                self.kernels[k] -= self.learning_rate * dk[k]/(np.sqrt(self.prev_change[k])+self.epsilon)\n",
    "        if self.momentum:\n",
    "            for k in range(num):\n",
    "                self.kernels[k] -= self.eta * self.prev_change[k]\n",
    "                self.prev_change[k] = dk[k] * self.learning_rate\n",
    "        return np.array(grad_in_overall, dtype = object)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 28, 28, 4)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = conv2D(4,(5,5,4))\n",
    "x = np.random.randn(28,28,4)\n",
    "y = np.random.randn(28,28,4)\n",
    "a= l.forward(np.array([x,y]))\n",
    "l.backprop(np.array([x,y]), np.array([y,x])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class max_pooling3D(Layer):\n",
    "    def __init__(self, stride : ty.Tuple[int, int], size : ty.Tuple[int,int]):\n",
    "        \"\"\"Initialising the stride\"\"\"\n",
    "        self.stride = stride\n",
    "    \n",
    "    def forward(self, input : np.ndarray):\n",
    "        \"\"\"Forward pass for 3D max pooling layer\"\"\"\n",
    "        N, rows, columns, channels = input.shape\n",
    "        out = input.reshape(N,rows//self.stride[0], self.stride[0], columns//self.stride[1], self.stride[1], channels).max(axis=(2,4))\n",
    "        return out\n",
    "    def backprop(self, input : np.ndarray, grad_out):\n",
    "        \"\"\"\n",
    "        Taken grad input from next layer, it will make the grad 0 where ever\n",
    "        the input in window is not maximum\n",
    "        example\n",
    "        grad_out is N x 7 x 7 x 4\n",
    "        this should be made into N x 14 x 14 x 4\n",
    "        We will achieve this by assigning this gradient to arg max indices\n",
    "        \"\"\"\n",
    "        N, rows, columns, channels = input.shape\n",
    "        grad_in = np.zeros_like(input)\n",
    "        for n in range(N):\n",
    "            for i in range(rows//self.stride[0]):\n",
    "                for j in range(columns//self.stride[1]):\n",
    "                    for c in range(channels):\n",
    "                        window = input[i,2*i:2*i+2, 2*j : 2*j+2, c]\n",
    "                        d = np.unravel_index(window.argmax(), window.shape)\n",
    "                        grad_in[n,2*i+d[0],2*j+d[1],c] = grad_out[n,i,j,c]\n",
    "        return grad_in\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class flatten(Layer):\n",
    "    def __init__(self, dim_in:ty.Tuple[int,int,int]) -> None:\n",
    "        self.dim_in = dim_in\n",
    "\n",
    "    def forward(self,  input : np.ndarray):\n",
    "        \"\"\"forward pass flatten\n",
    "        The input image will be N x 7 x 7 x 4\"\"\"\n",
    "        N = input.shape[0]\n",
    "        # since the flattening layer will flatten the\n",
    "        # 7 x 7 x 4 to 196\n",
    "        return input.reshape(N, np.product(self.dim_in))\n",
    "    \n",
    "    def backprop(self, input : np.ndarray, grad_out):\n",
    "        \"\"\"\n",
    "        Backprop in this flattening layer is basically\n",
    "        reshaping the gradient, here grad_out is N x 196\n",
    "        matrix, we will reshape it to N x 7 x 7 x 4\n",
    "        \"\"\"\n",
    "        return grad_out.reshape(input.shape)    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Layer):\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def forward(self, input : np.ndarray):\n",
    "        \"\"\"forward pass relu function\"\"\"\n",
    "        return relu(input)\n",
    "    \n",
    "    def backprop(self, input, grad_out):\n",
    "        # ReLU gradient is 0 when input is negative, 1 otherwise\n",
    "        return grad_out * (input > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    def __init__(self, dimensions, learning_rate=0.001, momentum = False, eta = 0.9, rmsprop = False, rho = 0.9):\n",
    "        \"\"\"\n",
    "        This function returns x.T W + b\\\\\n",
    "        where W is the weight matrix,\\\\\n",
    "        b is the bias vector\\\\\n",
    "        These variables are defined for the class\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        # *dimensions : * unpacks the tuple\n",
    "        self.weights = np.random.normal(0, np.sqrt(2/np.product(dimensions)), dimensions) \n",
    "        self.bias = np.zeros(dimensions[1])\n",
    "        self.W_prev_grad = np.zeros(dimensions)\n",
    "        self.b_prev_grad = np.zeros(dimensions[1])\n",
    "        self.eta = eta\n",
    "        self.rmsprop = rmsprop\n",
    "        self.rho = rho\n",
    "        self.epsilon = 1e-9\n",
    "\n",
    "    def forward(self,input):\n",
    "        \"\"\"\n",
    "        This function returns XW + b\\\\\n",
    "        where W is the weight matrix,\\\\\n",
    "        b is the bias vector\\\\\n",
    "        These variables are defined for the class\n",
    "        \"\"\"\n",
    "        return input @ self.weights + self.bias\n",
    "    \n",
    "    def backprop(self, input, grad_out):\n",
    "        \"\"\"\n",
    "        We are given the gradient computed till this layer, we have\n",
    "        to backpropagate it from this layer.\\\\\n",
    "        This will be \\\\\n",
    "        grad_out = dL/d(XW + b) \\\\\n",
    "        further, \\\\\n",
    "        dL/dx = dL/d(XW+b) d(XW+b)/dx \\\\\n",
    "        dL/dx = grad_out * W.T\n",
    "        \"\"\"\n",
    "        # gradient to be passed to previous layer\n",
    "        grad_in = grad_out @ self.weights.T\n",
    "        \n",
    "        # compute gradient w.r.t. weights and bias\n",
    "        # input.T       dim_in x N\n",
    "        # grad_out   N x dim_out\n",
    "        grad_weights = input.T @ grad_out\n",
    "        # we are taking sum along the dimensions to bring down the dimensionality from 2D to a vector\n",
    "        # essentially, we are adding the different gradients together.\n",
    "        grad_bias = grad_out.sum(axis=0)\n",
    "        \n",
    "        # grad_weights is sum of gradients over the batch\n",
    "        # so is grad_bias, it is also the sum of gradients over the batch.\n",
    "        if not self.rmsprop:\n",
    "            self.weights = self.weights - self.learning_rate * grad_weights\n",
    "            self.biases = self.bias - self.learning_rate * grad_bias\n",
    "        if self.rmsprop:\n",
    "            self.W_prev_grad = self.rho * self.W_prev_grad + (1-self.rho)*grad_weights**2\n",
    "            self.b_prev_grad = self.rho * self.b_prev_grad + (1-self.rho)*grad_bias**2\n",
    "            self.weights -= self.learning_rate * grad_weights/(np.sqrt(self.W_prev_grad)+self.epsilon)\n",
    "            self.bias -= self.learning_rate * grad_bias/(np.sqrt(self.b_prev_grad)+self.epsilon)\n",
    "            \n",
    "\n",
    "        if self.momentum:\n",
    "            self.weights -= self.eta * self.W_prev_grad\n",
    "            self.W_prev_grad = self.learning_rate* grad_weights\n",
    "            self.bias -= self.eta * self.b_prev_grad\n",
    "            self.b_prev_grad = self.learning_rate * grad_bias\n",
    "\n",
    "        # returning the gradient for the previous layer\n",
    "        return grad_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_crossentropy_with_logits(logits, reference_answers):\n",
    "    # Compute crossentropy from logits[batch,n_classes] and ids of correct answers\n",
    "    logits_for_answers = logits[np.arange(len(logits)),reference_answers]\n",
    "    xentropy = - logits_for_answers + np.log(np.sum(np.exp(logits),axis=-1))    \n",
    "    return xentropy\n",
    "    \n",
    "def grad_softmax_crossentropy_with_logits(logits,reference_answers):\n",
    "    # Compute crossentropy gradient from logits[batch,n_classes] and ids of correct answers\n",
    "    ones_for_answers = np.zeros_like(logits)\n",
    "    ones_for_answers[np.arange(len(logits)),reference_answers] = 1\n",
    "    \n",
    "    softmax = np.exp(logits) / np.exp(logits).sum(axis=-1,keepdims=True)\n",
    "    \n",
    "    return (- ones_for_answers + softmax) / logits.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(network, X):\n",
    "    # Compute activations of all network layers by applying them sequentially.\n",
    "    # Return a list of activations for each layer. \n",
    "    \n",
    "    activations = []\n",
    "    input = X\n",
    "    # Looping through each layer\n",
    "    for l in network:\n",
    "        activations.append(l.forward(input))\n",
    "        # Updating input to last layer output\n",
    "        input = activations[-1]\n",
    "    return activations\n",
    "    \n",
    "def predict(network,X):\n",
    "    # Compute network predictions. Returning indices of largest Logit probability\n",
    "    logits = forward(network,X)[-1]\n",
    "    return logits.argmax(axis=-1)\n",
    "\n",
    "__predict__ = lambda network,X : softmax(forward(network,X)[-1])\n",
    "\n",
    "\n",
    "def train(network,X,y):\n",
    "    # Train our network on a given batch of X and y.\n",
    "    # We first need to run forward to get all layer activations.\n",
    "    # Then we can run layer.backward going from last to first layer.\n",
    "    # After we have called backward for all layers, all Dense layers have already made one gradient step.\n",
    "    \n",
    "    \n",
    "    # Get the layer activations\n",
    "    layer_activations = forward(network, X)\n",
    "    layer_inputs = [X]+layer_activations  #layer_input[i] is an input for network[i]\n",
    "    logits = layer_activations[-1]\n",
    "    \n",
    "    # Compute the loss and the initial gradient\n",
    "    loss = softmax_crossentropy_with_logits(logits,y)\n",
    "    loss_grad = grad_softmax_crossentropy_with_logits(logits,y)\n",
    "    \n",
    "    # Propagate gradients through the network\n",
    "    # Reverse propogation as this is backprop\n",
    "    for layer_index in range(len(network))[::-1]:\n",
    "        layer = network[layer_index]\n",
    "        loss_grad = layer.backprop(layer_inputs[layer_index],loss_grad) #grad w.r.t. input, also weight updates\n",
    "        \n",
    "    return np.mean(loss)\n",
    "\n",
    "from tqdm import trange\n",
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.random.permutation(len(inputs))\n",
    "    for start_idx in trange(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla SGD , batch size = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 66/66 [00:44<00:00,  1.48it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/cmaspi/study/AI2100/assignment-4/ai20btech11006-hw4.ipynb Cell 15'\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cmaspi/study/AI2100/assignment-4/ai20btech11006-hw4.ipynb#ch0000014?line=22'>23</a>\u001b[0m train_accuracy_log\u001b[39m.\u001b[39mappend(np\u001b[39m.\u001b[39mmean(predict(network,X_train)\u001b[39m==\u001b[39my_train))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cmaspi/study/AI2100/assignment-4/ai20btech11006-hw4.ipynb#ch0000014?line=23'>24</a>\u001b[0m val_accuracy_log\u001b[39m.\u001b[39mappend(np\u001b[39m.\u001b[39mmean(predict(network,X_val)\u001b[39m==\u001b[39my_val))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/cmaspi/study/AI2100/assignment-4/ai20btech11006-hw4.ipynb#ch0000014?line=24'>25</a>\u001b[0m train_log\u001b[39m.\u001b[39mappend(np\u001b[39m.\u001b[39mmean(cross_entropy(y_train, __predict__(network,X_train))))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cmaspi/study/AI2100/assignment-4/ai20btech11006-hw4.ipynb#ch0000014?line=25'>26</a>\u001b[0m val_log\u001b[39m.\u001b[39mappend(np\u001b[39m.\u001b[39mmean(cross_entropy(y_val, __predict__(network,X_val))))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cmaspi/study/AI2100/assignment-4/ai20btech11006-hw4.ipynb#ch0000014?line=27'>28</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mEpoch\u001b[39m\u001b[39m\"\u001b[39m,epoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[1;32m/home/cmaspi/study/AI2100/assignment-4/ai20btech11006-hw4.ipynb Cell 13'\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(network, X)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cmaspi/study/AI2100/assignment-4/ai20btech11006-hw4.ipynb#ch0000012?line=15'>16</a>\u001b[0m     logits \u001b[39m=\u001b[39m forward(network,X)[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cmaspi/study/AI2100/assignment-4/ai20btech11006-hw4.ipynb#ch0000012?line=16'>17</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m logits\u001b[39m.\u001b[39margmax(axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/cmaspi/study/AI2100/assignment-4/ai20btech11006-hw4.ipynb#ch0000012?line=18'>19</a>\u001b[0m __predict__ \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m network,X : softmax(forward(network,X)[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cmaspi/study/AI2100/assignment-4/ai20btech11006-hw4.ipynb#ch0000012?line=21'>22</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain\u001b[39m(network,X,y):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cmaspi/study/AI2100/assignment-4/ai20btech11006-hw4.ipynb#ch0000012?line=22'>23</a>\u001b[0m     \u001b[39m# Train our network on a given batch of X and y.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cmaspi/study/AI2100/assignment-4/ai20btech11006-hw4.ipynb#ch0000012?line=23'>24</a>\u001b[0m     \u001b[39m# We first need to run forward to get all layer activations.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cmaspi/study/AI2100/assignment-4/ai20btech11006-hw4.ipynb#ch0000012?line=27'>28</a>\u001b[0m     \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cmaspi/study/AI2100/assignment-4/ai20btech11006-hw4.ipynb#ch0000012?line=28'>29</a>\u001b[0m     \u001b[39m# Get the layer activations\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cmaspi/study/AI2100/assignment-4/ai20btech11006-hw4.ipynb#ch0000012?line=29'>30</a>\u001b[0m     layer_activations \u001b[39m=\u001b[39m forward(network, X)\n",
      "\u001b[1;32m/home/cmaspi/study/AI2100/assignment-4/ai20btech11006-hw4.ipynb Cell 13'\u001b[0m in \u001b[0;36mforward\u001b[0;34m(network, X)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/cmaspi/study/AI2100/assignment-4/ai20btech11006-hw4.ipynb#ch0000012?line=6'>7</a>\u001b[0m \u001b[39m# Looping through each layer\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/cmaspi/study/AI2100/assignment-4/ai20btech11006-hw4.ipynb#ch0000012?line=7'>8</a>\u001b[0m \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m network:\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/cmaspi/study/AI2100/assignment-4/ai20btech11006-hw4.ipynb#ch0000012?line=8'>9</a>\u001b[0m     activations\u001b[39m.\u001b[39mappend(l\u001b[39m.\u001b[39;49mforward(\u001b[39minput\u001b[39;49m))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cmaspi/study/AI2100/assignment-4/ai20btech11006-hw4.ipynb#ch0000012?line=9'>10</a>\u001b[0m     \u001b[39m# Updating input to last layer output\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cmaspi/study/AI2100/assignment-4/ai20btech11006-hw4.ipynb#ch0000012?line=10'>11</a>\u001b[0m     \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m activations[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "\u001b[1;32m/home/cmaspi/study/AI2100/assignment-4/ai20btech11006-hw4.ipynb Cell 6'\u001b[0m in \u001b[0;36mconv2D.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cmaspi/study/AI2100/assignment-4/ai20btech11006-hw4.ipynb#ch0000005?line=44'>45</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(W\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m+\u001b[39m\u001b[39m2\u001b[39m\u001b[39m*\u001b[39mpadding\u001b[39m-\u001b[39mk,):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cmaspi/study/AI2100/assignment-4/ai20btech11006-hw4.ipynb#ch0000005?line=45'>46</a>\u001b[0m     \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(H\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m+\u001b[39m\u001b[39m2\u001b[39m\u001b[39m*\u001b[39mpadding\u001b[39m-\u001b[39mk):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/cmaspi/study/AI2100/assignment-4/ai20btech11006-hw4.ipynb#ch0000005?line=46'>47</a>\u001b[0m         temp[x, y] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49msum(padded_array[i:i\u001b[39m+\u001b[39;49mk,j:j\u001b[39m+\u001b[39;49mk,:]\u001b[39m*\u001b[39;49mkernel)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cmaspi/study/AI2100/assignment-4/ai20btech11006-hw4.ipynb#ch0000005?line=47'>48</a>\u001b[0m         y\u001b[39m+\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cmaspi/study/AI2100/assignment-4/ai20btech11006-hw4.ipynb#ch0000005?line=48'>49</a>\u001b[0m     x\u001b[39m+\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36msum\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numpy/core/fromnumeric.py:2296\u001b[0m, in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   <a href='file:///home/cmaspi/.local/lib/python3.10/site-packages/numpy/core/fromnumeric.py?line=2292'>2293</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m out\n\u001b[1;32m   <a href='file:///home/cmaspi/.local/lib/python3.10/site-packages/numpy/core/fromnumeric.py?line=2293'>2294</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m res\n\u001b[0;32m-> <a href='file:///home/cmaspi/.local/lib/python3.10/site-packages/numpy/core/fromnumeric.py?line=2295'>2296</a>\u001b[0m \u001b[39mreturn\u001b[39;00m _wrapreduction(a, np\u001b[39m.\u001b[39;49madd, \u001b[39m'\u001b[39;49m\u001b[39msum\u001b[39;49m\u001b[39m'\u001b[39;49m, axis, dtype, out, keepdims\u001b[39m=\u001b[39;49mkeepdims,\n\u001b[1;32m   <a href='file:///home/cmaspi/.local/lib/python3.10/site-packages/numpy/core/fromnumeric.py?line=2296'>2297</a>\u001b[0m                       initial\u001b[39m=\u001b[39;49minitial, where\u001b[39m=\u001b[39;49mwhere)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numpy/core/fromnumeric.py:86\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///home/cmaspi/.local/lib/python3.10/site-packages/numpy/core/fromnumeric.py?line=82'>83</a>\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='file:///home/cmaspi/.local/lib/python3.10/site-packages/numpy/core/fromnumeric.py?line=83'>84</a>\u001b[0m             \u001b[39mreturn\u001b[39;00m reduction(axis\u001b[39m=\u001b[39maxis, out\u001b[39m=\u001b[39mout, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpasskwargs)\n\u001b[0;32m---> <a href='file:///home/cmaspi/.local/lib/python3.10/site-packages/numpy/core/fromnumeric.py?line=85'>86</a>\u001b[0m \u001b[39mreturn\u001b[39;00m ufunc\u001b[39m.\u001b[39;49mreduce(obj, axis, dtype, out, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpasskwargs)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "network = []\n",
    "network.append(conv2D(4,(5,5,1), momentum=False))\n",
    "network.append(ReLU())\n",
    "network.append(max_pooling3D((2,2),(2,2)))\n",
    "network.append(conv2D(4,(5,5,4), momentum = False))\n",
    "network.append(ReLU())\n",
    "network.append(max_pooling3D((2,2),(2,2)))\n",
    "network.append(flatten((7,7,4)))\n",
    "network.append(Dense((196,49), momentum = False))\n",
    "network.append(ReLU())\n",
    "network.append(Dense((49,10), momentum=False))\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "        \n",
    "train_log = []\n",
    "val_log = []\n",
    "train_accuracy_log = []\n",
    "val_accuracy_log = []\n",
    "for epoch in range(15):\n",
    "    for x_batch,y_batch in iterate_minibatches(X_train,y_train,batchsize=15,shuffle=True):\n",
    "        train(network,x_batch,y_batch)\n",
    "    \n",
    "    train_accuracy_log.append(np.mean(predict(network,X_train)==y_train))\n",
    "    val_accuracy_log.append(np.mean(predict(network,X_val)==y_val))\n",
    "    train_log.append(np.mean(cross_entropy(y_train, __predict__(network,X_train))))\n",
    "    val_log.append(np.mean(cross_entropy(y_val, __predict__(network,X_val))))\n",
    "    \n",
    "    print(\"Epoch\",epoch+1)\n",
    "    print(\"Train loss:\",train_log[-1])\n",
    "    print(\"Test loss:\",val_log[-1])\n",
    "    print(\"Train accuracy:\",train_accuracy_log[-1])\n",
    "    print(\"Test accuracy:\",val_accuracy_log[-1])\n",
    "    \n",
    "    if epoch in [0,14]:\n",
    "        logits = __predict__(network, X_val)\n",
    "        X_embedded = TSNE(n_components=2, learning_rate='auto', init='random',perplexity=10, n_iter = 10000).fit_transform(logits)\n",
    "        fig, ax = plt.subplots()\n",
    "        for i in range(10):\n",
    "            ax.scatter(X_embedded[10*i:10*i+10, 0], X_embedded[10*i:10*i+10, 1], label=f'{y_val[10*i]}')\n",
    "        ax.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    single_image = np.array([X_train[0]])\n",
    "    layer_activation = forward(network, single_image)\n",
    "    fig,ax = plt.subplots(2,4, figsize=(12,3))\n",
    "    ax[0][0].imshow(layer_activation[0][:,:,:,0].reshape(28,28), cmap = 'gray')\n",
    "    ax[0][1].imshow(layer_activation[0][:,:,:,1].reshape(28,28), cmap = 'gray')\n",
    "    ax[0][2].imshow(layer_activation[0][:,:,:,2].reshape(28,28), cmap = 'gray')\n",
    "    ax[0][3].imshow(layer_activation[0][:,:,:,3].reshape(28,28), cmap = 'gray')\n",
    "    ax[1][0].imshow(layer_activation[3][:,:,:,0].reshape(14,14), cmap = 'gray')\n",
    "    ax[1][1].imshow(layer_activation[3][:,:,:,1].reshape(14,14), cmap = 'gray')\n",
    "    ax[1][2].imshow(layer_activation[3][:,:,:,2].reshape(14,14), cmap = 'gray')\n",
    "    ax[1][3].imshow(layer_activation[3][:,:,:,3].reshape(14,14), cmap = 'gray')\n",
    "    plt.show()\n",
    "\n",
    "plt.plot(train_log,label='train loss')\n",
    "plt.plot(val_log,label='test loss')\n",
    "plt.legend(loc='best')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Momentum , batch size = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = []\n",
    "network.append(conv2D(4,(5,5,1), momentum=True))\n",
    "network.append(ReLU())\n",
    "network.append(max_pooling3D((2,2),(2,2)))\n",
    "network.append(conv2D(4,(5,5,4), momentum = True))\n",
    "network.append(ReLU())\n",
    "network.append(max_pooling3D((2,2),(2,2)))\n",
    "network.append(flatten((7,7,4)))\n",
    "network.append(Dense((196,49), momentum = True))\n",
    "network.append(ReLU())\n",
    "network.append(Dense((49,10), momentum=True))\n",
    "\n",
    "train_log = []\n",
    "val_log = []\n",
    "train_accuracy_log = []\n",
    "val_accuracy_log = []\n",
    "for epoch in range(15):\n",
    "    for x_batch,y_batch in iterate_minibatches(X_train,y_train,batchsize=15,shuffle=True):\n",
    "        train(network,x_batch,y_batch)\n",
    "    \n",
    "    train_accuracy_log.append(np.mean(predict(network,X_train)==y_train))\n",
    "    val_accuracy_log.append(np.mean(predict(network,X_val)==y_val))\n",
    "    train_log.append(np.mean(cross_entropy(y_train, __predict__(network,X_train))))\n",
    "    val_log.append(np.mean(cross_entropy(y_val, __predict__(network,X_val))))\n",
    "    \n",
    "    print(\"Epoch\",epoch+1)\n",
    "    print(\"Train loss:\",train_log[-1])\n",
    "    print(\"Test loss:\",val_log[-1])\n",
    "    print(\"Train accuracy:\",train_accuracy_log[-1])\n",
    "    print(\"Test accuracy:\",val_accuracy_log[-1])\n",
    "    \n",
    "    if epoch in [0,14]:\n",
    "        logits = __predict__(network, X_val)\n",
    "        X_embedded = TSNE(n_components=2, learning_rate='auto', init='random',perplexity=10, n_iter = 10000).fit_transform(logits)\n",
    "        fig, ax = plt.subplots()\n",
    "        for i in range(10):\n",
    "            ax.scatter(X_embedded[10*i:10*i+10, 0], X_embedded[10*i:10*i+10, 1], label=f'{y_val[10*i]}')\n",
    "        ax.legend()\n",
    "        plt.show()\n",
    "\n",
    "    single_image = np.array([X_train[0]])\n",
    "    layer_activation = forward(network, single_image)\n",
    "    fig,ax = plt.subplots(2,4, figsize=(12,3))\n",
    "    ax[0][0].imshow(layer_activation[0][:,:,:,0].reshape(28,28), cmap = 'gray')\n",
    "    ax[0][1].imshow(layer_activation[0][:,:,:,1].reshape(28,28), cmap = 'gray')\n",
    "    ax[0][2].imshow(layer_activation[0][:,:,:,2].reshape(28,28), cmap = 'gray')\n",
    "    ax[0][3].imshow(layer_activation[0][:,:,:,3].reshape(28,28), cmap = 'gray')\n",
    "    ax[1][0].imshow(layer_activation[3][:,:,:,0].reshape(14,14), cmap = 'gray')\n",
    "    ax[1][1].imshow(layer_activation[3][:,:,:,1].reshape(14,14), cmap = 'gray')\n",
    "    ax[1][2].imshow(layer_activation[3][:,:,:,2].reshape(14,14), cmap = 'gray')\n",
    "    ax[1][3].imshow(layer_activation[3][:,:,:,3].reshape(14,14), cmap = 'gray')\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "plt.plot(train_log,label='train loss')\n",
    "plt.plot(val_log,label='test loss')\n",
    "plt.legend(loc='best')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RMSPROP, batch size = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = []\n",
    "network.append(conv2D(4,(5,5,1), rmsprop = True))\n",
    "network.append(ReLU())\n",
    "network.append(max_pooling3D((2,2),(2,2)))\n",
    "network.append(conv2D(4,(5,5,4), rmsprop = True))\n",
    "network.append(ReLU())\n",
    "network.append(max_pooling3D((2,2),(2,2)))\n",
    "network.append(flatten((7,7,4)))\n",
    "network.append(Dense((196,49), rmsprop = True))\n",
    "network.append(ReLU())\n",
    "network.append(Dense((49,10), rmsprop = True))\n",
    "\n",
    "        \n",
    "train_log = []\n",
    "val_log = []\n",
    "train_accuracy_log = []\n",
    "val_accuracy_log = []\n",
    "for epoch in range(15):\n",
    "    for x_batch,y_batch in iterate_minibatches(X_train,y_train,batchsize=15,shuffle=True):\n",
    "        train(network,x_batch,y_batch)\n",
    "    \n",
    "    train_accuracy_log.append(np.mean(predict(network,X_train)==y_train))\n",
    "    val_accuracy_log.append(np.mean(predict(network,X_val)==y_val))\n",
    "    train_log.append(np.mean(cross_entropy(y_train, __predict__(network,X_train))))\n",
    "    val_log.append(np.mean(cross_entropy(y_val, __predict__(network,X_val))))\n",
    "    \n",
    "    print(\"Epoch\",epoch+1)\n",
    "    print(\"Train loss:\",train_log[-1])\n",
    "    print(\"Test loss:\",val_log[-1])\n",
    "    print(\"Train accuracy:\",train_accuracy_log[-1])\n",
    "    print(\"Test accuracy:\",val_accuracy_log[-1])\n",
    "    \n",
    "    if epoch in [0,14]:\n",
    "        logits = __predict__(network, X_val)\n",
    "        X_embedded = TSNE(n_components=2, learning_rate='auto', init='random',perplexity=10, n_iter = 10000).fit_transform(logits)\n",
    "        fig, ax = plt.subplots()\n",
    "        for i in range(10):\n",
    "            ax.scatter(X_embedded[10*i:10*i+10, 0], X_embedded[10*i:10*i+10, 1], label=f'{y_val[10*i]}')\n",
    "        ax.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    single_image = np.array([X_train[0]])\n",
    "    layer_activation = forward(network, single_image)\n",
    "    fig,ax = plt.subplots(2,4, figsize=(12,3))\n",
    "    ax[0][0].imshow(layer_activation[0][:,:,:,0].reshape(28,28), cmap = 'gray')\n",
    "    ax[0][1].imshow(layer_activation[0][:,:,:,1].reshape(28,28), cmap = 'gray')\n",
    "    ax[0][2].imshow(layer_activation[0][:,:,:,2].reshape(28,28), cmap = 'gray')\n",
    "    ax[0][3].imshow(layer_activation[0][:,:,:,3].reshape(28,28), cmap = 'gray')\n",
    "    ax[1][0].imshow(layer_activation[3][:,:,:,0].reshape(14,14), cmap = 'gray')\n",
    "    ax[1][1].imshow(layer_activation[3][:,:,:,1].reshape(14,14), cmap = 'gray')\n",
    "    ax[1][2].imshow(layer_activation[3][:,:,:,2].reshape(14,14), cmap = 'gray')\n",
    "    ax[1][3].imshow(layer_activation[3][:,:,:,3].reshape(14,14), cmap = 'gray')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plt.plot(train_log,label='train loss')\n",
    "plt.plot(val_log,label='test loss')\n",
    "plt.legend(loc='best')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla SGD, batch size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = []\n",
    "network.append(conv2D(4,(5,5,1), momentum=False))\n",
    "network.append(ReLU())\n",
    "network.append(max_pooling3D((2,2),(2,2)))\n",
    "network.append(conv2D(4,(5,5,4), momentum = False))\n",
    "network.append(ReLU())\n",
    "network.append(max_pooling3D((2,2),(2,2)))\n",
    "network.append(flatten((7,7,4)))\n",
    "network.append(Dense((196,49), momentum = False))\n",
    "network.append(ReLU())\n",
    "network.append(Dense((49,10), momentum=False))\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "        \n",
    "train_log = []\n",
    "val_log = []\n",
    "train_accuracy_log = []\n",
    "val_accuracy_log = []\n",
    "for epoch in range(15):\n",
    "    for x_batch,y_batch in iterate_minibatches(X_train,y_train,batchsize=50,shuffle=True):\n",
    "        train(network,x_batch,y_batch)\n",
    "    \n",
    "    train_accuracy_log.append(np.mean(predict(network,X_train)==y_train))\n",
    "    val_accuracy_log.append(np.mean(predict(network,X_val)==y_val))\n",
    "    train_log.append(np.mean(cross_entropy(y_train, __predict__(network,X_train))))\n",
    "    val_log.append(np.mean(cross_entropy(y_val, __predict__(network,X_val))))\n",
    "    \n",
    "    print(\"Epoch\",epoch+1)\n",
    "    print(\"Train loss:\",train_log[-1])\n",
    "    print(\"Test loss:\",val_log[-1])\n",
    "    print(\"Train accuracy:\",train_accuracy_log[-1])\n",
    "    print(\"Test accuracy:\",val_accuracy_log[-1])\n",
    "    \n",
    "    if epoch in [0,14]:\n",
    "        logits = __predict__(network, X_val)\n",
    "        X_embedded = TSNE(n_components=2, learning_rate='auto', init='random',perplexity=10, n_iter = 10000).fit_transform(logits)\n",
    "        fig, ax = plt.subplots()\n",
    "        for i in range(10):\n",
    "            ax.scatter(X_embedded[10*i:10*i+10, 0], X_embedded[10*i:10*i+10, 1], label=f'{y_val[10*i]}')\n",
    "        ax.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    single_image = np.array([X_train[0]])\n",
    "    layer_activation = forward(network, single_image)\n",
    "    fig,ax = plt.subplots(2,4, figsize=(12,3))\n",
    "    ax[0][0].imshow(layer_activation[0][:,:,:,0].reshape(28,28), cmap = 'gray')\n",
    "    ax[0][1].imshow(layer_activation[0][:,:,:,1].reshape(28,28), cmap = 'gray')\n",
    "    ax[0][2].imshow(layer_activation[0][:,:,:,2].reshape(28,28), cmap = 'gray')\n",
    "    ax[0][3].imshow(layer_activation[0][:,:,:,3].reshape(28,28), cmap = 'gray')\n",
    "    ax[1][0].imshow(layer_activation[3][:,:,:,0].reshape(14,14), cmap = 'gray')\n",
    "    ax[1][1].imshow(layer_activation[3][:,:,:,1].reshape(14,14), cmap = 'gray')\n",
    "    ax[1][2].imshow(layer_activation[3][:,:,:,2].reshape(14,14), cmap = 'gray')\n",
    "    ax[1][3].imshow(layer_activation[3][:,:,:,3].reshape(14,14), cmap = 'gray')\n",
    "    plt.show()\n",
    "\n",
    "plt.plot(train_log,label='train loss')\n",
    "plt.plot(val_log,label='test loss')\n",
    "plt.legend(loc='best')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Momentum, batch size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = []\n",
    "network.append(conv2D(4,(5,5,1), momentum=True))\n",
    "network.append(ReLU())\n",
    "network.append(max_pooling3D((2,2),(2,2)))\n",
    "network.append(conv2D(4,(5,5,4), momentum = True))\n",
    "network.append(ReLU())\n",
    "network.append(max_pooling3D((2,2),(2,2)))\n",
    "network.append(flatten((7,7,4)))\n",
    "network.append(Dense((196,49), momentum = True))\n",
    "network.append(ReLU())\n",
    "network.append(Dense((49,10), momentum=True))\n",
    "\n",
    "train_log = []\n",
    "val_log = []\n",
    "train_accuracy_log = []\n",
    "val_accuracy_log = []\n",
    "for epoch in range(15):\n",
    "    for x_batch,y_batch in iterate_minibatches(X_train,y_train,batchsize=50,shuffle=True):\n",
    "        train(network,x_batch,y_batch)\n",
    "    \n",
    "    train_accuracy_log.append(np.mean(predict(network,X_train)==y_train))\n",
    "    val_accuracy_log.append(np.mean(predict(network,X_val)==y_val))\n",
    "    train_log.append(np.mean(cross_entropy(y_train, __predict__(network,X_train))))\n",
    "    val_log.append(np.mean(cross_entropy(y_val, __predict__(network,X_val))))\n",
    "    \n",
    "    print(\"Epoch\",epoch+1)\n",
    "    print(\"Train loss:\",train_log[-1])\n",
    "    print(\"Test loss:\",val_log[-1])\n",
    "    print(\"Train accuracy:\",train_accuracy_log[-1])\n",
    "    print(\"Test accuracy:\",val_accuracy_log[-1])\n",
    "    \n",
    "    if epoch in [0,14]:\n",
    "        logits = __predict__(network, X_val)\n",
    "        X_embedded = TSNE(n_components=2, learning_rate='auto', init='random',perplexity=10, n_iter = 10000).fit_transform(logits)\n",
    "        fig, ax = plt.subplots()\n",
    "        for i in range(10):\n",
    "            ax.scatter(X_embedded[10*i:10*i+10, 0], X_embedded[10*i:10*i+10, 1], label=f'{y_val[10*i]}')\n",
    "        ax.legend()\n",
    "        plt.show()\n",
    "\n",
    "    single_image = np.array([X_train[0]])\n",
    "    layer_activation = forward(network, single_image)\n",
    "    fig,ax = plt.subplots(2,4, figsize=(12,3))\n",
    "    ax[0][0].imshow(layer_activation[0][:,:,:,0].reshape(28,28), cmap = 'gray')\n",
    "    ax[0][1].imshow(layer_activation[0][:,:,:,1].reshape(28,28), cmap = 'gray')\n",
    "    ax[0][2].imshow(layer_activation[0][:,:,:,2].reshape(28,28), cmap = 'gray')\n",
    "    ax[0][3].imshow(layer_activation[0][:,:,:,3].reshape(28,28), cmap = 'gray')\n",
    "    ax[1][0].imshow(layer_activation[3][:,:,:,0].reshape(14,14), cmap = 'gray')\n",
    "    ax[1][1].imshow(layer_activation[3][:,:,:,1].reshape(14,14), cmap = 'gray')\n",
    "    ax[1][2].imshow(layer_activation[3][:,:,:,2].reshape(14,14), cmap = 'gray')\n",
    "    ax[1][3].imshow(layer_activation[3][:,:,:,3].reshape(14,14), cmap = 'gray')\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "plt.plot(train_log,label='train loss')\n",
    "plt.plot(val_log,label='test loss')\n",
    "plt.legend(loc='best')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RMSPROP, batch size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = []\n",
    "network.append(conv2D(4,(5,5,1), rmsprop = True))\n",
    "network.append(ReLU())\n",
    "network.append(max_pooling3D((2,2),(2,2)))\n",
    "network.append(conv2D(4,(5,5,4), rmsprop = True))\n",
    "network.append(ReLU())\n",
    "network.append(max_pooling3D((2,2),(2,2)))\n",
    "network.append(flatten((7,7,4)))\n",
    "network.append(Dense((196,49), rmsprop = True))\n",
    "network.append(ReLU())\n",
    "network.append(Dense((49,10), rmsprop = True))\n",
    "\n",
    "        \n",
    "train_log = []\n",
    "val_log = []\n",
    "train_accuracy_log = []\n",
    "val_accuracy_log = []\n",
    "for epoch in range(15):\n",
    "    for x_batch,y_batch in iterate_minibatches(X_train,y_train,batchsize=50,shuffle=True):\n",
    "        train(network,x_batch,y_batch)\n",
    "    \n",
    "    train_accuracy_log.append(np.mean(predict(network,X_train)==y_train))\n",
    "    val_accuracy_log.append(np.mean(predict(network,X_val)==y_val))\n",
    "    train_log.append(np.mean(cross_entropy(y_train, __predict__(network,X_train))))\n",
    "    val_log.append(np.mean(cross_entropy(y_val, __predict__(network,X_val))))\n",
    "    \n",
    "    print(\"Epoch\",epoch+1)\n",
    "    print(\"Train loss:\",train_log[-1])\n",
    "    print(\"Test loss:\",val_log[-1])\n",
    "    print(\"Train accuracy:\",train_accuracy_log[-1])\n",
    "    print(\"Test accuracy:\",val_accuracy_log[-1])\n",
    "    \n",
    "    if epoch in [0,14]:\n",
    "        logits = __predict__(network, X_val)\n",
    "        X_embedded = TSNE(n_components=2, learning_rate='auto', init='random',perplexity=10, n_iter = 10000).fit_transform(logits)\n",
    "        fig, ax = plt.subplots()\n",
    "        for i in range(10):\n",
    "            ax.scatter(X_embedded[10*i:10*i+10, 0], X_embedded[10*i:10*i+10, 1], label=f'{y_val[10*i]}')\n",
    "        ax.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    single_image = np.array([X_train[0]])\n",
    "    layer_activation = forward(network, single_image)\n",
    "    fig,ax = plt.subplots(2,4, figsize=(12,3))\n",
    "    ax[0][0].imshow(layer_activation[0][:,:,:,0].reshape(28,28), cmap = 'gray')\n",
    "    ax[0][1].imshow(layer_activation[0][:,:,:,1].reshape(28,28), cmap = 'gray')\n",
    "    ax[0][2].imshow(layer_activation[0][:,:,:,2].reshape(28,28), cmap = 'gray')\n",
    "    ax[0][3].imshow(layer_activation[0][:,:,:,3].reshape(28,28), cmap = 'gray')\n",
    "    ax[1][0].imshow(layer_activation[3][:,:,:,0].reshape(14,14), cmap = 'gray')\n",
    "    ax[1][1].imshow(layer_activation[3][:,:,:,1].reshape(14,14), cmap = 'gray')\n",
    "    ax[1][2].imshow(layer_activation[3][:,:,:,2].reshape(14,14), cmap = 'gray')\n",
    "    ax[1][3].imshow(layer_activation[3][:,:,:,3].reshape(14,14), cmap = 'gray')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plt.plot(train_log,label='train loss')\n",
    "plt.plot(val_log,label='test loss')\n",
    "plt.legend(loc='best')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
