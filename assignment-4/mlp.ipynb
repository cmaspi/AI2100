{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import typing as ty\n",
    "import torchvision\n",
    "\n",
    "# dark background\n",
    "plt.style.use('dark_background')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = torchvision.datasets.MNIST(\"files/\")\n",
    "test = torchvision.datasets.MNIST(\"files/\", train = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dict = { i:[] for i in range(10) }\n",
    "test_dict = { i:[] for i in range(10) }\n",
    "\n",
    "for image, label in train:\n",
    "    train_dict[label].append(np.array(image).flatten())\n",
    "\n",
    "for image, label in test:\n",
    "    test_dict[label].append(np.array(image).flatten())\n",
    "\n",
    "plt.imshow(train_dict[0][0].reshape((28,28)), cmap = 'gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activation functions\n",
    "\n",
    "def sigmoid(x : ty.Union[float, np.ndarray]) -> ty.Union[float, np.ndarray]:\n",
    "    \"\"\"sigmoid function\n",
    "    \n",
    "    returns 1/(1+exp(-x))\"\"\"\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def tanh(x: ty.Union[float, np.ndarray]) -> ty.Union[float, np.ndarray]:\n",
    "    \"\"\"tanh function\n",
    "    \n",
    "    returns tanh(x)\"\"\"\n",
    "    return np.tanh(x)\n",
    "\n",
    "def relu(x : ty.Union[float, np.ndarray]) -> ty.Union[float, np.ndarray]:\n",
    "    \"\"\"ReLU function\n",
    "    \n",
    "    returns max(0,x)\"\"\"\n",
    "    return np.maximum(0,x)\n",
    "    \n",
    "def softmax(x : np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Softmax function\n",
    "    \n",
    "    return [e^x_i/sum(e^x_i) for all i]\"\"\"\n",
    "    denom = np.sum(np.exp(x), axis = -1)\n",
    "    return np.exp(x)/denom\n",
    "\n",
    "\n",
    "\n",
    "# loss function\n",
    "\n",
    "def entropy(pmf: np.ndarray) -> float:\n",
    "    \"\"\"Returns the entropy of the given pmf \n",
    "\n",
    "    The formula used is : -sum p(x) log p(x)\n",
    "\n",
    "    Example\n",
    "    --------\n",
    "    >>> p = np.array([0.5, 0.5])\n",
    "    >>> entropy(p)\n",
    "    1.0\n",
    "    \"\"\"\n",
    "    pmf[pmf==0] = 1e-8\n",
    "    return -np.sum(pmf*np.log2(pmf))\n",
    "\n",
    "def KL_divergence(p: np.ndarray, q: np.ndarray) -> float:\n",
    "    \"\"\"This function returns the KL divergence between the two given pmfs\n",
    "    \n",
    "    The formula used is : sum p(x) log p(x)/q(x) \n",
    "    \n",
    "    Example\n",
    "    -------\n",
    "    >>> p = np.array([0.5, 0.5])\n",
    "    >>> q = np.array([0.6, 0.4])\n",
    "    >>> KL_divergence(p, q)\n",
    "    0.029446844526784283\n",
    "    \"\"\"\n",
    "    q[q==0] = 1e-8\n",
    "    p[p==0] = 1e-8\n",
    "    return np.sum(p * np.log2(p/q)) \n",
    "\n",
    "def cross_entropy(p: np.ndarray, q:np.ndarray) -> float:\n",
    "    \"\"\"This function returns the cross entropy betwen the two given pmfs\n",
    "    \n",
    "    The formula used is : H(p) + D(p||q)\n",
    "    \n",
    "    Example\n",
    "    -------\n",
    "    >>> p = np.array([0.5, 0.5])\n",
    "    >>> q = np.array([0.6, 0.4])\n",
    "    >>> cross_entropy(p, q)\n",
    "    1.0294468445267844\n",
    "    \"\"\"\n",
    "    return entropy(p)+KL_divergence(p, q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = np.ones((3,8))\n",
    "# sigmoid(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activation functions derivatives\n",
    "\n",
    "def sigmoid_der(x : ty.Union[float, np.ndarray]) -> ty.Union[float, np.ndarray]:\n",
    "    \"\"\"sigmoid function derivative\"\"\"    \n",
    "    return np.exp(-x)/(1+np.exp(-x))**2\n",
    "\n",
    "def tanh_der(x: ty.Union[float, np.ndarray]) -> ty.Union[float, np.ndarray]:\n",
    "    \"\"\"tanh function derivative\"\"\"\n",
    "    return 1 - np.tanh(x)**2\n",
    "\n",
    "def relu_der(x : ty.Union[float, np.ndarray]) -> ty.Union[float, np.ndarray]:\n",
    "    \"\"\"ReLU function derivative\"\"\"\n",
    "    return (x>=0).astype(int)\n",
    "    \n",
    "def softmax_der(x : np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Softmax function derivative\"\"\"\n",
    "    denom = np.sum(np.exp(x))\n",
    "    return np.exp(x)/denom\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mlp:\n",
    "    @classmethod\n",
    "    def __init__(self) -> None:\n",
    "        self.layers = []\n",
    "        self.loss_func = None\n",
    "        self.weights = []\n",
    "        self.bias = []\n",
    "        self.batch_size = 50\n",
    "        self.lr = None\n",
    "\n",
    "    @staticmethod\n",
    "    def shuffle(input :np.ndarray, labels:np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Shuffles the data\"\"\"\n",
    "        l = len(input)\n",
    "        permutation = np.random.permutation(l)\n",
    "        input = input[permutation]\n",
    "        labels = labels[permutation]\n",
    "        return input, labels\n",
    "\n",
    "    @classmethod\n",
    "    def addLayer(self, out_size : int, activation_func : str, in_size : int = None):\n",
    "        \"\"\"\n",
    "        out_size is the size of output from this layer\n",
    "        in_size is the size of input, it should be defined for the input layer\n",
    "        \"\"\"\n",
    "        dispatcher = {\"sigmoid\": sigmoid, \"tanh\":tanh, \"softmax\": softmax, \"relu\": relu}\n",
    "        self.layers.append((out_size, dispatcher[activation_func], in_size))\n",
    "\n",
    "    @classmethod\n",
    "    def compile(self, loss_func : str, learning_rate : int):\n",
    "        \"\"\"\n",
    "        This function compiles the program, as in adds weights to the layers\n",
    "        \"\"\"\n",
    "        factor = 10000\n",
    "        dispatcher = {\"cross_entropy\": cross_entropy}\n",
    "        self.loss_func = dispatcher[loss_func]\n",
    "        self.lr = learning_rate\n",
    "        W = np.random.random(size = (self.layers[0][0], self.layers[0][2]))/factor\n",
    "        self.weights.append(W)\n",
    "        self.bias.append(np.random.random(size = (self.layers[0][0], ))/factor)\n",
    "        for idx in range(1, len(self.layers)):\n",
    "            prevlayer = self.layers[idx-1]\n",
    "            layer = self.layers[idx]\n",
    "            W = np.random.random(size = (layer[0], prevlayer[0]))/factor\n",
    "            self.weights.append(W)\n",
    "            bias = np.random.random(size = (layer[0],))/factor\n",
    "            self.bias.append(bias)\n",
    "        self.weights = np.array(self.weights, dtype = object)\n",
    "        self.bias = np.array(self.bias, dtype = object)\n",
    "    \n",
    "    @classmethod\n",
    "    def forwardpass(self, input: np.ndarray):\n",
    "        ret = [input]\n",
    "        for weight, bias, layer in zip(self.weights, self.bias, self.layers):\n",
    "            try:\n",
    "                input = (weight @ input) + bias\n",
    "            except:\n",
    "                print(weight.shape, input.shape, bias.shape)\n",
    "                raise ValueError(\"dhfih\")\n",
    "            activation_func = layer[1]\n",
    "            input = activation_func(input)\n",
    "            ret.append(input)\n",
    "        return ret\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def fit(self, X: np.ndarray, Y: np.ndarray, epochs = 10, num_classes = 10) -> None:\n",
    "        \"\"\"\n",
    "        This function will do backpropgation for given number of epochs\n",
    "        Y should be array of scalars\n",
    "        \"\"\"\n",
    "        X,Y = self.shuffle(X,Y)\n",
    "        train = X[:int(X.shape[0]*0.8)]\n",
    "        labels = Y[:int(X.shape[0]*0.8)]\n",
    "        batches = np.arange(0, len(train), self.batch_size)\n",
    "        \n",
    "        dispatcher = {sigmoid : sigmoid_der}\n",
    "\n",
    "        # number of layers\n",
    "        num_layers = self.layers.__len__()\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            print(\"Epoch: #{}\".format(epoch+1))\n",
    "\n",
    "            # shuffling the data\n",
    "            train, labels = self.shuffle(train, labels)\n",
    "\n",
    "            grad_weights = [0] * num_layers #np.zeros(num_layers)\n",
    "            grad_bias = [0] * num_layers #np.zeros(num_layers)\n",
    "\n",
    "            for idx in batches: \n",
    "                batch_X = train[idx : idx+self.batch_size]\n",
    "                batch_label  = labels[idx : idx+self.batch_size]\n",
    "                for x,y in zip(batch_X, batch_label):\n",
    "                    x_layers = self.forwardpass(x)\n",
    "                    delta = [0] * num_layers #np.zeros(num_layers)\n",
    "                    one_hot = np.zeros(num_classes)\n",
    "                    one_hot[y] = 1\n",
    "                    one_hot = y\n",
    "                    delta[-1] = x_layers[-1] - one_hot\n",
    "\n",
    "                    grad_weights[-1] += delta[-1]@x_layers[-1].T\n",
    "                    grad_bias[-1] += delta[-1]\n",
    "\n",
    "                    for layer in range(num_layers-2,-1,-1):\n",
    "                        delta[layer] = ((self.weights[layer+1].T @ delta[layer+1]) * dispatcher[self.layers[layer][1]](self.weights[layer]@x_layers[layer]+self.bias[layer]))\n",
    "                        delta[layer].shape = (delta[layer].shape[0],1)\n",
    "                        x_layers[layer].shape = (x_layers[layer].shape[0],1)\n",
    "                        grad_weights[layer] += delta[layer][0]@x_layers[layer].T\n",
    "                        grad_bias[layer] += delta[layer]\n",
    "                        # return grad_weights[layer].shape, x_layers[layer].shape,\n",
    "                        \n",
    "                grad_weights = [q/self.batch_size for q in grad_weights]\n",
    "                grad_bias = [q/self.batch_size for q in grad_bias]\n",
    "                \n",
    "                \n",
    "                temp1 = [self.lr * q for q in grad_weights]\n",
    "                temp2 = [self.lr * q for q in grad_bias]\n",
    "\n",
    "                \n",
    "                self.weights -= np.array(temp1, dtype = object)\n",
    "                self.bias -= np.array(temp2, dtype = object)\n",
    "                    \n",
    "                    \n",
    "\n",
    "            loss = self.evaluate(X[int(X.shape[0]*0.8):], Y[int(X.shape[0]*0.8):])\n",
    "            print(f'Loss : {loss}')\n",
    "    @classmethod\n",
    "    def evaluate(self, x, y):\n",
    "        p = []\n",
    "        for xx in x:\n",
    "            p.append(self.predict(xx))\n",
    "\n",
    "        loss = 0\n",
    "        for i,yy in enumerate(y):\n",
    "            if p[i][yy]!=0:\n",
    "                loss -= yy*np.log(p[i][yy])\n",
    "        return loss\n",
    "\n",
    "    @classmethod\n",
    "    def predict(self, input : np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Returns predicted labels\"\"\"\n",
    "        for weight, bias, layer in zip(self.weights, self.bias, self.layers):\n",
    "            # print(input.shape, weight.shape)\n",
    "            input = (weight @ input.T).T + bias\n",
    "            activation_func = layer[1]\n",
    "            input = activation_func(input)\n",
    "        return input\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01\n"
     ]
    }
   ],
   "source": [
    "classifier = mlp()\n",
    "classifier.addLayer(49, 'sigmoid', 784)\n",
    "classifier.addLayer(10, 'softmax', 49)\n",
    "classifier.compile('cross_entropy', 0.01)\n",
    "# aaa = np.array([train_dict[0][0], train_dict[1][0]])\n",
    "# print(aaa.shape)\n",
    "# classifier.forwardpass(aaa[0])\n",
    "print(classifier.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: #1\n",
      "(10, 49) (49, 49) (10,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "dhfih",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_179979/3839807631.py\u001b[0m in \u001b[0;36mforwardpass\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m                 \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (10,49) (10,) ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_179979/1697033870.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mxx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0myy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m100\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_179979/3839807631.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, Y, epochs, num_classes)\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[0mbatch_label\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m                     \u001b[0mx_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforwardpass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m                     \u001b[0mdelta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_layers\u001b[0m \u001b[0;31m#np.zeros(num_layers)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m                     \u001b[0mone_hot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_179979/3839807631.py\u001b[0m in \u001b[0;36mforwardpass\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dhfih\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m             \u001b[0mactivation_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivation_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: dhfih"
     ]
    }
   ],
   "source": [
    "xx = []\n",
    "for i in range(10):\n",
    "    xx.extend(train_dict[i][:100])\n",
    "xx = np.array(xx)\n",
    "yy = np.array([i//100 for i in range(1000)]).astype(int)\n",
    "classifier.fit(xx,yy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
